\chapter{Conclusion and Future Directions}\label{ch:conclusion}

This thesis explored three crucial dimensions of machine learning: \textit{modeling, training, and theory}. Each dimension was addressed through dedicated projects, contributing to the advancement of machine learning methodologies and applications across diverse domains.

Chapter~\ref{ch:fmri}, focusing on the theme of data modeling, introduced Shared Gaussian Process Factor Analysis (S-GPFA) as a novel probabilistic model for analyzing fMRI data. S-GPFA effectively captures the shared temporal dynamics and spatial organization of brain activity across individuals in multi-subject fMRI datasets, enabling researchers to uncover common neural patterns while accounting for individual differences. By incorporating Gaussian Process priors to model temporal correlations within fMRI data, S-GPFA provides a more interpretable representation of brain activity compared to traditional static methods. The model's ability to simultaneously perform functional aggregation, dimensionality reduction, and dynamical modeling makes it a powerful tool for analyzing multi-subject fMRI datasets.

Chapter~\ref{ch:sgc} addressed the theme of distributed training and the challenge of mitigating the impact of stragglers on the training process. The chapter introduced two novel coding schemes, Selective Reattempt Sequential Gradient Coding (SR-SGC) and Multiplexed Sequential Gradient Coding (M-SGC), that leverage coding across both spatial and temporal dimensions to achieve straggler resilience with reduced computational load. These schemes exploit the temporal diversity of straggler behavior, where periods of high straggler activity are often followed by periods of low straggler activity, to trade off computation and training time. Experiments on a large-scale AWS Lambda cluster demonstrated the effectiveness of M-SGC in significantly reducing runtime and improving training performance compared to other schemes.

Chapter~\ref{ch:mecb} investigated the information theoretical foundations of coupling and compression, aligning with the theme of theory. The chapter introduced Minimum Entropy Coupling with Bottleneck (MEC-B) as a framework for lossy data compression under logarithmic loss. MEC-B extends the classical Minimum Entropy Coupling (MEC) framework by incorporating a bottleneck that regulates the degree of stochasticity or determinism in the coupling. The chapter also introduced the Entropy-Bounded Information Maximization (EBIM) formulation for compression and proposed a novel search algorithm for identifying deterministic mappings with guaranteed performance bounds. Experiments on Markov Coding Games with rate limits illustrated the practical application of MEC-B.

\section{Future Directions}

\subsection{Group Temporal Dynamics
Analysis in fMRI}

While S-GPFA offers a promising approach for analyzing fMRI data, there are several avenues for future research to enhance its capabilities and broaden its applications. One limitation of S-GPFA is its scalability with respect to the number of time samples in fMRI recordings. As fMRI datasets continue to grow in size and complexity, it becomes increasingly important to develop methods that can efficiently handle large-scale data. One potential solution is to explore techniques for dividing long recordings into smaller chunks and feeding them as multiple samples to a single S-GPFA model. This approach could improve the model's computational efficiency and enable it to handle longer and more complex fMRI time series.

Another direction for future research is to incorporate Bayesian inference techniques into the S-GPFA framework. By adopting approximate Bayesian inference methods, it would be possible to draw uncertainty estimates for model parameters, such as subject-specific topographies and shared timescales. This would provide valuable insights into the reliability and variability of the model's estimates, enhancing the interpretability of the results and allowing researchers to better understand the confidence intervals associated with the identified neural patterns.

Furthermore, exploring the use of change-point and non-stationary kernels could improve S-GPFA's ability to capture complex temporal dynamics in fMRI data. Change-point kernels could help identify abrupt changes or transitions in brain activity patterns, while non-stationary kernels could account for gradual changes or trends over time. These advancements could provide a more nuanced and accurate representation of the dynamic nature of brain activity, potentially leading to new discoveries about the neural mechanisms underlying cognitive processes and psychiatric disorders.

\subsection{Sequential Gradient Coding}

Future research on sequential gradient coding could address the inherent privacy and security risks associated with data sharing in distributed computing frameworks. As distributed training involves sharing data and model updates across multiple servers, developing methods that protect sensitive information and ensure data security is crucial. One promising direction is to explore the integration of privacy-preserving techniques and private gradient computation into the proposed coding schemes.

Furthermore, incorporating gradient compression methods into the experimental setup could address communication constraints and improve the efficiency of distributed training. Gradient compression techniques, such as quantization or sparsification, can significantly reduce the amount of data that needs to be transmitted between servers, thereby alleviating communication bottlenecks and improving training speed. Integrating these techniques with SR-SGC and M-SGC could lead to significant practical improvements in the scalability and efficiency of distributed training for large-scale machine learning models.

\subsection{Minimum Entropy Coupling with Bottleneck}

Several avenues for future research could further enhance the capabilities and applications of the proposed solution for Minimum Entropy Coupling with Bottleneck (MEC-B). One important direction is to quantify the gap between the separate optimization of the encoder and decoder, as presented in this thesis, and the optimal joint setting.  Understanding the gap between separate and joint optimization could guide the development of methods that jointly optimize both components, potentially improving the overall performance of MEC-B and achieving higher fidelity with more efficient compression.

Another area for future exploration is enabling fine-grained control over the entropy spread in the coupling. As evident in Figure~\ref{ch3:fig:tradeoff}, the current solution for MEC-B controls the entropy of the coupling by mapping parts of the input to the output deterministically, while leaving others completely stochastic. Developing methods that allow for more flexible control over the spread of entropy within the coupling could expand the applicability of MEC-B to a wider range of problems.

Additionally, the application of Entropy-Bounded Information Maximization (EBIM) in watermarking language models, as demonstrated in \cite{kirchenbauer2023watermark}, suggests a valuable intersection with state-of-the-art AI applications. 

Moreover, extending this framework to continuous cases could lead to the design of neural network architectures based on the proposed model and provide information-theoretic insights into a broad spectrum of deep learning problems. These include unpaired sample-to-sample translation \cite{isola2016image, zhu2017unpaired, hoffman2018cycada}, joint compression and upscaling \cite{kang2019toward, liu2021lossy}, and the InfoMax framework \cite{tschannen2019mutual, hjelm2018learning}, among others.

A significant challenge in the continuous case is the issue of distribution permutations: within the current framework, solutions can be achieved by maximizing the mutual information between bijective functions of input and output. A straightforward example of this is swapping the channels of images in the input and output while still maintaining maximal mutual information. Proposing strategies to mitigate this issue is another potential direction for future research in the continuous domain.
